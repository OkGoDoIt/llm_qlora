model_name: anc_13b_v2
base_model: TheBloke/Llama-2-13B-fp16
model_family: llama  # if unspecified will use AutoModelForCausalLM/AutoTokenizer
model_context_window: 4096  # if unspecified will use tokenizer.model_max_length
data:
  type: vicuna
  dataset: ehartford/wizard_vicuna_70k_unfiltered  # HuggingFace hub
lora:
  r: 32
  lora_alpha: 32
  target_modules:  # modules for which to train lora adapters
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM
trainer:
  save_strategy: steps
  save_steps: 200
  save_total_limit: 4
  evaluation_strategy: steps
  eval_steps: 200
  batch_size: 3
  gradient_accumulation_steps: 8
  warmup_steps: 100
  num_train_epochs: 1
  learning_rate: 0.0002  # 2e-4
  logging_steps: 5
  bf16: True
  bits: 16
  report_to: "wandb"
trainer_output_dir: trainer_live_outputs/
model_output_dir: models/  # model saved in {model_output_dir}/{model_name}
